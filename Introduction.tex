\section{Introduction}
\label{sec:intro}
Conventional methods for grasp detection consider accurate estimation of 6D pose as a fundamental. These approaches typically consist in recovering correspondences between CAD models and object point clouds to register grasps from pre-computed database (\textcolor{cyan}{\cite{munoz2016fast}}, \textcolor{cyan}{\cite{zeng2017multi}}). The grasps are previously calculated for CAD models and then to be regressed to determine high-quality one for objects (\textcolor{cyan}{\cite{bohg2013data}}). Some approaches obtain 6D pose estimation by employing algorithms such as iterative closest point (\textcolor{cyan}{\cite{besl1992method}}) to align models to point clouds or exploiting local descriptors such as SIFT key points (\textcolor{cyan}{\cite{dias2014sift}}) for 3D matching. Whereas, recent approaches leverage the advance of deep learning technique to design end-to-end models for predicting 6DoF rotation and translation vector in 3D space (\textcolor{cyan}{\cite{wu20196d}}, \textcolor{cyan}{\cite{wang2019densefusion}}). In general, however, the heavy dependencies of these methods on 3D CAD models, which might be not always available for all objects or might require intensive labor in the pre-scanning process, restricts their ability to be widely applied in all scenarios.

This motivated alternative approaches that directly detect grasps from sensor data thanks to CNN without estimating object pose. In other words, instead of registering point clouds to a pre-computed dataset and indexing grasps, these methods learn to identify a set of candidates and the probability of success of grasps. In recent works, (\textcolor{cyan}{\cite{mahler2017dex}}, \textcolor{cyan}{\cite{mahler2018dex}}) apply deep CNNs to find features, while others (\textcolor{cyan}{\cite{redmon2015real}}, \textcolor{cyan}{\cite{lenz2015deep}}) develop end-to-end learning for estimating possible grasps. The results of learning-based approaches allow to grasp pre-known objects, which might be partially occluded, an unknown pose as well as fully novel objects.

Grasp detection area differs from object detection and pose estimation due to it requires both determining grasp candidates and maximining the probability of success of grasps. To generate quality candidates, which are understood as reasoning parts of objects for the gripper to appropriately operate, it seems to be inadequate to consider only 2D or 2.5D local features of objects since focusing on visual similarities might lead to failure in some cases, especially in occlusion context and texture-less objects. Therefore, a few approaches (\textcolor{cyan}{\cite{ten2017grasp}}, \textcolor{cyan}{\cite{mousavian20196}}, \textcolor{cyan}{\cite{liang2019pointnetgpd}}, \textcolor{cyan}{\cite{fang2020graspnet}}) take 3D geometry analysis into account to robust the performance of predicting grasp by localizing grasp from 3D point sets.

On the other hand, achieving highly feasible grasps is a challenging problem that researchers have to cope with. The successful grasps are considered to be not involved with undesirable contacts with surroundings. This is hardly achievable because of measurement noise in acquiring data, occlusion environments, and cluttered scenarios. In attempt to deal with this problem, several methods intensively collect numerous grasp candidates, while others propose end-to-end models for 6-DOF grasp detection. These end-to-end approaches achieve state-of-the-art result in benchmarks thanks to employing backbone networks such as PoinNet++ (\textcolor{cyan}{\cite{qi2017pointnet++}}) to take advantage of local property. Although these strategies could gain high-grade grasping points, they find hard to avoid unexpected collisions. Solely considering regional information of each candidate without examining the spatial relationship of neighboring candidates appears to be insufficient for system to be fully aware of context. Thus, the contextual information is a prospective material to be succeed in accomplishing collision-free grasps.

Motivated by the above perspective, we introduce an effective end-to-end model for 6-DOF grasp detection. It accomplishes highly competitive grasp configurations in severe scenarios of occlusion when compared with others in benchmarks. This success is attributed to the essence of leveraging both a voting mechanism architecture (\textcolor{cyan}{\cite{qi2019deep}}) to elect candidates and a context learning module to encode the spatial relationship of neighboring candidates into feature vectors. Voting mechanism allows our model to widen our model's capability to beforehand unknown objects. Besides, fusing contextual spatial features into local features enables our method to enhance the performance in alleviating potential collision. The following highlights the main contributions of our work:

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{itemize}
	\item Our research proposes a new robust framework VoteGrasp combing voting mechanism and contextual learning module for 6-DOF grasp detection and achieves remarkable results while operating in severe scenes of occlusions. Besides, our proposal demonstrates its generalization capability to novel objects.
	\item We develop a context learning module that contributes the spatial dependency of objects in candidatesâ€™ vicinity to the feature vectors to learn collision-free grasps. 
	\item Experiments compare the results of attention modules to find out what is most robust and suitable to our model.
\end{itemize}
