\section{Related work}
\label{sec:related_work}

\subsection{3D Point Cloud Based Grasp Detection}
Robotic grasping is conventionally involved in two related problems of perception and planning. The perceptual component aims to acquire the position and orientation of the object to be grasped. The planning component considers how to evaluate a good manipulation. This primitive idea comes to 3D retrieval methods that retrieve segmented point clouds to 3D CAD models to estimate the object poses and then decide grasps from a pre-defined grasp dataset. As mentioned in previous sections, this old-fashioned approach is impractical in some cases due to the problem of the existence of all accurate CAD models. Furthermore, these methods could not detec grasps for novel objects outside of the dataset. 

To overcome these issues, machine learning-based approaches have been introduced to directly detect grasp from sensor data without estimating object pose with a conventional pipeline of grasp sampling process, extracting features of the grasps, and evaluating the quality of grasps. (\textcolor{cyan}{\cite{ten2017grasp}}) proposed grasp pose detection (GPD) algorithm from point clouds. Unlike (\textcolor{cyan}{\cite{herzog2012template}}), which necessarily segments the object from the background, this method first identifies a ROI that could include multiple objects or even background to find a grasp. The large set of grasp candidates is interested to be found in ROI with respect to two conditions of not being in collision with the point cloud and there being no contact between grippers and point cloud. Then, a four-layer CNN decides whether or not a candidate is a grasp for observed and occluded surfaces acquired from the depth sensor. As an extension of PGD idea, PointNetGPD (\textcolor{cyan}{\cite{liang2019pointnetgpd}}) replaces the CNN-based evaluation model with a new network based on architecture of PointNet (\textcolor{cyan}{\cite{qi2017pointnet}}). Taking advantage of PointNet architecture, this evaluation model can directly perform geometry analysis, which the original PGD idea lacks, from the 3D point cloud and therefore can detect more reliable grasps. However, these approaches could not provide good enough grasp assessments, they need exhaustively manual sampling grasps, which sometimes is hard to acquire when the raw point cloud is spare. This motivates the method (\textcolor{cyan}{\cite{mousavian20196}}) to introduce two network architectures for both sampling and evaluating grasps. Instead of manually sampling grasps, this one utilizes a variational auto-encoder (VAE) model to generate a diverse set of grasps as well as limit the number of failing grasps. Detecting grasps is important because not all grasps are kinematically feasible or collision-free for the manipulation of the robot. In terms of the evaluation model, this method not only classifies each grasp but also iteratively refines a significant portion of the rejected grasps to the successful ones. Nevertheless, all these approaches solely depend on the local visible parts of objects, which occasionally is imperfect due to the noisy depth value. Some methods sort to use high-quality depth sensors or utilize the multi-view technique (\textcolor{cyan}{\cite{ten2017grasp}}) to provide high-success rates of grasps.

The lack of geometric information about candidates’ surroundings and information about the scene mitigates the performance of the above methods. Perceiving this remaining issue, end-to-end methods (\textcolor{cyan}{\cite{fang2020graspnet}}, \textcolor{cyan}{\cite{ni2020pointnet++}}) take the whole point clouds as input and neglect the tradition pipeline of time-consuming grasp sampling process. They combine global data information to directly predict the poses and qualities of spatial grasps. Their PointNet++-based architectures allow them to immediately extract local spatial features from the raw data point clouds, while others such as (\textcolor{cyan}{\cite{choi2018learning}}) employs a three-dimensional deep learning neural network to deal with voxelized point clouds, which might be less precise in detail due to voxelization. Although considering whole point clouds, these approaches don’t take the relationship between objects into account and only focus on local representations of points. As a result, their performance in cluttered environments is unreliable. We address the remaining challenges by leveraging voting mechanisms combined with contextual information to ensure the generalization and reliability of grasps.

\subsection{Hough Voting in Object Detection}
The Hough Transform was originally informed to detect defined shapes in 2D space such as lines, circles, or eclipses (\textcolor{cyan}{\cite{hough1959machine}}, \textcolor{cyan}{\cite{hough1962method}}, \textcolor{cyan}{\cite{duda1972textordfeminineuse}}). This technique is limited to shapes characterized by a small number of parameters. The Generalized Hough Transform is then introduced to extend the application of the primitive algorithm to arbitrary shapes. It, therefore, is widely extended and applied to computer vision tasks including object detection (\textcolor{cyan}{\cite{gall2013class}}, \textcolor{cyan}{\cite{gall2011hough}}), motion detection (\textcolor{cyan}{\cite{gall2011hough}}, \textcolor{cyan}{\cite{kalviainen1996motion}}), medical imaging (\textcolor{cyan}{\cite{golemati2006comparison}}), and robot navigation (\textcolor{cyan}{\cite{iocchi2001probabilistic}}). In terms of 3D scenes, methods (\textcolor{cyan}{\cite{deng2018ppfnet}}, \textcolor{cyan}{\cite{rabbani2005efficient}}) utilizes the original Hough Transform formulation in a straightforward way to deal with 3D analytical shapes like spheres and cylinders. However, these methods cannot be applied to generic free-form objects, which are common in realistic applications.

More recently, several methods (\textcolor{cyan}{\cite{silberberg1984iterative}}, \textcolor{cyan}{\cite{tombari2010object}}) widen the use of Hough voting mechanisms to 3D object detection. (\textcolor{cyan}{\cite{tombari2010object}}) even proves the robustness of their approach in scenes with a significant degree of occlusions. They aim to deploy 3D features of interesting point, which is chosen randomly or extracted by means of feature detectors, to compute the correspondences between 3D models and the current scene. The features of each point do not normally consist of 3D properties but include points’ relative spatial relationship with respect to centroids of 3D models. In that way, correspondences can cast a vote in 3D Hough space to accumulate evidence for feasible centroids in the scene. If enough features vote for the presence of the centroid of an object, the object is determined. Though this method performs well in cluttered scenarios, it requires the existence of 3D models, which are not always available in practice.

Deep learning technique allows (\textcolor{cyan}{\cite{kehl2016deep}}) to generalize the use of voting mechanisms to novel objects. In the training phase, this research densely samples scale-invariant RGB-D patches from synthetic views of fixed size. Features of each patch is learned by CNNs and local votes describing the patch 3D center point offset to the object centroid are stored together into a codebook. In the practicing phase, patches from real data are fed into neural networks to regress features for a k-NN search in a pre-computed codebook. If the feature distance of retrieved nearest neighbors of patches is smaller than a certain thresh, these patches are understood to cast 6D votes. This work furthermore uses a vote filtering process to refine votes and reject implausible ones, so that detection results are more reliable.

A proposed end-to-end model VoteNet (\textcolor{cyan}{\cite{qi2019deep}}) recently reaches state-of-the-art results in 3D point cloud detection from real 3D scans because of some main reasons. Firstly, it directly learns 3D features from raw point cloud data by adopting PointNet++ backbone to output a set of seed points. Whereas, (\textcolor{cyan}{\cite{liu2016ssd}}) learns 2D descriptors from RGB-D images and (\textcolor{cyan}{\cite{song2016deep}}, \textcolor{cyan}{\cite{hou20193d}}) require regularizing point clouds such as voxelization to learn features so that they ignore or sacrifice sufficient spatial information. Secondly, while (\textcolor{cyan}{\cite{kehl2016deep}}) determines votes by looking up a pre-computed codebook, VoteNet generates votes by leveraging a shared deep network-based voting module. This approach is more efficient due to votes are trained jointly with the rest of the pipeline compared with (\textcolor{cyan}{\cite{kehl2016deep}}) storing votes of each patch of images independently into a codebook. All these improvements allow this model to directly vote for virtual centroids of objects and achieve high-quality 3D object proposals. Inspired by the success of VoteNet, we leverage its voting architecture to strengthen grasp detection to occlusion.

\subsection{Context and Attention in 3D point clouds}
Contextual information is essential to be precisely aware of a particular location. Much research employs the use of contextual perception to improve the performance of computer vision tasks in 3D scenarios such as 3D point matching (\textcolor{cyan}{\cite{deng2018ppfnet}}), point cloud semantic segmentation (\textcolor{cyan}{\cite{ye20183d}}), instance segmentation of 3D point clouds (\textcolor{cyan}{\cite{hu2018semantic}}), and 3D scene layout prediction (\textcolor{cyan}{\cite{shi2019hierarchy}}). Differ from conventional methods, which purely take local geometric features, these methods fuse global features including points and normals within a local vicinity into learned local descriptors to produce more discriminative local representations. As a result, challenging tasks for 3D perception are robustly solved. However, numerous methods making the use of contextual information equally treat neighbors of a location so that they cannot precisely reflect on the relationships between points in 3D contexts.

In order to cope with above issues, the attention mechanism is leveraged to meticulously investigate the dependency of a local position on each neighbor. In other words, instead of assuming that all neighbors have the same impact on a local representation, attention mechanism computes how much each neighbor affects a particular local feature. (\textcolor{cyan}{\cite{xie2018attentional}}) proposes ShapeContextNet which combines the attention idea with the concept of shape context to be applied in point cloud classification and segmentation. The shape context idea designs a discriminative descriptor with spatially inhomogeneous cells. This descriptor is actually a feature vector (histogram) that captures neighborhood information by counting the number of neighboring points in each cell. The descriptor is combined with self-attention idea to inform Attentional ShapeContextNet, which is the main contribution of this research. In terms of place recognition, (\textcolor{cyan}{\cite{zhang2019pcan}}) informs Point Contextual Attention Network to effectively handle this problem. This method utilizes contextual information and per-point local feature by adding attention networks to PointNetVLAD to produce the attention map that estimates an attention score for each point. By leveraging attention mechanism, this network can predict the significance of each local point and therefore pay more attention to the most informative points.  (\textcolor{cyan}{\cite{paigwar2019attentional}}) perceives that although PointNet performs a fascinating result in 3D object detection, it is limited to the points in point clouds. Thus, this method develops Attentional PointNet for 3D object detection in spacious contexts taken from the LiDAR sensor. Instead of processing the whole point cloud, it learns to find possible locations of objects of interest. Utilizing attention mechanism facilitates this approach to sequentially attend to relevant smaller regions in a large point cloud so that meets the significance of both detection results and inference time. These successes inspire us to stipulate that incorporating contextual information and attention theory is prospective to our problem of interest. 